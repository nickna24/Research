# ğŸš¢ Titanic ML Pipeline with HTCondor DAGMan

This project demonstrates how to structure a **machine learning workflow** using **HTCondor DAGMan**, with the [Titanic dataset](https://www.kaggle.com/c/titanic).  
It serves as a lightweight skeleton for modular ML pipelines â€” similar to autonomous vehicle or GAN workflows â€” showing how to chain preprocessing, training, and evaluation jobs via DAG dependencies.

---

## ğŸ§© Pipeline Overview

| Stage | Script | Purpose | Output |
|-------|---------|----------|---------|
| **Preprocess** | `scripts/preprocess_data.py` | Cleans raw CSVs, encodes features, saves `.npy` arrays + metadata | `data/preprocessed/*` |
| **Train** | `scripts/train_model.py` | Trains a small MLP with variable hyperparameters | `checkpoints/model_*.pt` |
| **Evaluate** | `scripts/evaluate_model.py` | Loads a checkpoint and computes validation metrics | `logs/metrics_*.txt` |

Each stage runs as its own Condor job, chained by DAGMan.

---

## Directory Structure

titanic_dagman_project/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ train.csv
â”‚ â”œâ”€â”€ test.csv
â”‚ â””â”€â”€ preprocessed/
â”‚ â”œâ”€â”€ train_proc.npy
â”‚ â”œâ”€â”€ test_proc.npy
â”‚ â”œâ”€â”€ labels.npy
â”‚ â””â”€â”€ meta.json
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ preprocess_data.py
â”‚ â”œâ”€â”€ train_model.py
â”‚ â””â”€â”€ evaluate_model.py
â”œâ”€â”€ submit/
â”‚ â”œâ”€â”€ preprocess.sub
â”‚ â”œâ”€â”€ train.sub
â”‚ â””â”€â”€ eval.sub
â”œâ”€â”€ checkpoints/
â”œâ”€â”€ logs/
â”œâ”€â”€ out/
â”œâ”€â”€ err/
â””â”€â”€ workflow.dag


## ğŸ”„ DAG Workflow

### `workflow.dag`
```dag
# --- Stage 1: Preprocess ---
JOB PREPROC submit/preprocess.sub

# --- Stage 2: Train ---
JOB TRAIN1 submit/train.sub
VARS TRAIN1 OPT="adam" LR="0.001" WPRED="1.0" WCONST="0.0005"

JOB TRAIN2 submit/train.sub
VARS TRAIN2 OPT="sgd" LR="0.01" WPRED="1.0" WCONST="0.0005"

PARENT PREPROC CHILD TRAIN1 TRAIN2

# --- Stage 3: Evaluate ---
JOB EVAL1 submit/eval.sub
VARS EVAL1 OPT="adam" LR="0.001"

JOB EVAL2 submit/eval.sub
VARS EVAL2 OPT="sgd" LR="0.01"

PARENT TRAIN1 CHILD EVAL1
PARENT TRAIN2 CHILD EVAL2
Workflow Summary:
PREPROC â†’ TRAIN(Adam, SGD) â†’ EVAL(Adam, SGD)
```

###  Model & Hyperparameters
A 3-layer MLP for binary classification (Survived vs Not Survived):

python
```
nn.Linear(in_dim, 64) â†’ ReLU â†’ nn.Linear(64, 32) â†’ ReLU â†’ nn.Linear(32, 1)
```
### Arguments passed from Condor submit files:

Arg	Description	Example
--optimizer	Optimizer type	adam, sgd
--lr	Learning rate	0.001, 0.01
--w_pred	Weight for BCE loss	1.0
--w_const	Weight for L2 regularization	0.0005
--checkpoint_path	Path to save model	checkpoints/model_adam_0.001.pt

### Checkpoints
Each training job outputs:

php-template
Copy code
checkpoints/model_<OPT>_<LR>.pt
Each checkpoint stores:

python
```
{
  "state_dict": model weights,
  "meta": {
    "optimizer": "adam",
    "lr": 0.001,
    "w_pred": 1.0,
    "w_const": 0.0005,
    "feature_cols": [...],
    "num_means": {...},
    "num_stds": {...},
    "cat_domains": {...},
    "val_indices": [...],
    "best_val_acc": 0.89
  }
}
```

Evaluation jobs read this file to compute metrics.

### Evaluation Outputs
Each EVAL job produces:

File	Location	Description
metrics_$(OPT)_$(LR).txt	logs/	Validation loss & accuracy
submission.csv	logs/	Optional Kaggle-style predictions
.out, .err, .log	out/, err/, logs/	Condor job outputs

Example metric file:

makefile
Copy code
val_loss: 0.451623
val_acc: 0.892857
âš™ï¸ GPU Configuration
Both training and evaluation jobs request one GPU:

sub
Copy code
request_gpus = 1
requirements = (CUDADeviceName =!= UNDEFINED)
Each job uses:

1 CPU core

1 GPU

2 GB RAM

Remove GPU lines if your pool has only CPUs.

### Running the Workflow
From the project root:

bash
```
condor_submit_dag workflow.dag
```

Monitor progress:

bash
```
condor_q
condor_q -dag
```

Artifacts are written to:

logs/ â€” logs, metrics, submission

checkpoints/ â€” trained model files

out/ & err/ â€” stdout/stderr for Condor jobs

ğŸ“š Artifact Flow
Stage	Input	Output
Preprocess	Raw CSVs	.npy files + meta.json
Train	Preprocessed data	.pt checkpoint
Evaluate	Checkpoint	Metrics + submission CSV

### Example Output Tree
pgsql
Copy code
checkpoints/
â”œâ”€â”€ model_adam_0.001.pt
â”œâ”€â”€ model_sgd_0.01.pt

logs/
â”œâ”€â”€ preprocess.log
â”œâ”€â”€ train_adam_0.001.log
â”œâ”€â”€ eval_adam_0.001.log
â”œâ”€â”€ metrics_adam_0.001.txt
â”œâ”€â”€ metrics_sgd_0.01.txt
â””â”€â”€ submission.csv

out/
â”œâ”€â”€ train_adam_0.001.out
â””â”€â”€ eval_adam_0.001.out

err/
â”œâ”€â”€ train_adam_0.001.err
â””â”€â”€ eval_adam_0.001.err

### Extending the Pipeline
Add sweeps: create new TRAIN + EVAL pairs in workflow.dag.

GANs: split generator/discriminator training into separate DAG nodes.

Autonomous vehicle ML: replace preprocess_data.py with a script that handles sensor or image data.

Metrics aggregation: add a post-evaluation DAG stage to parse and summarize metrics automatically.